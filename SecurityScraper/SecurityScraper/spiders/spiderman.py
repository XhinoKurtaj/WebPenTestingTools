import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy.http import Request
from scrapy.selector import Selector
from SecurityScraper.items import SecurityscraperItem
import re


class SpidermanSpider(CrawlSpider):
    name = 'spiderman'
    allowed_domains = ['scrapy.com']
    start_urls = ['https://scrapy.org/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        hxs = Selector(response)
        item = {}

        # CODE for scraping Forms
        forms = hxs.xpath('//form/@action').extract()
        for form in forms:
            formy = SecurityscraperItem()
            formy["form"] = form
            formy["location_url"] = response.url
            yield formy

        # CODE for scraping emails
        emails = hxs.xpath("//*[contains(text(),'@')]").extract()
        for email in emails:
            com = SecurityscraperItem()
            com["email"] = email
            com["location_url"] = response.url
            yield com

        # CODE for scraping comments
        comments = hxs.xpath('//comment()').extract()
        for comment in comments:
            com = SecurityscraperItem()
            com["comments"] = comment
            com["location_url"] = response.url
            yield com

        visited_links = []
        links = hxs.xpath('//a/@href').extract()
        link_validator = re.compile(
            "^(?:http|https):\/\/(?:[\w\.\-\+]+:{0,1}[\w\.\-\+]*@)?(?:[a-z0-9\-\.]+)(?::[0-9]+)?(?:\/|\/(?:[\w#!:\.\?\+=&amp;%@!\-\/\(\)]+)|\?(?:[\w#!:\.\?\+=&amp;%@!\-\/\(\)]+))?$")

        for link in links:
            if link_validator.match(link) and not link in visited_links:
                visited_links.append(link)
                yield Request(link, self.parse_item)
            else:
                full_url = response.urljoin(link)
                visited_links.append(full_url)
                yield Request(full_url, self.parse_item)
